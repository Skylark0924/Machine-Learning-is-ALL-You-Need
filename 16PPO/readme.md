# PG & TRPO & PPO & DPPO
> Policy based Sequential Decision

> åˆ«çœ‹åº•ä¸‹æœ‰è‹±æ–‡ï¼ŒçœŸçš„å¾ˆç®€å•ï¼Œä¸ä¿¡ä½ è¯»ğŸ˜‰

åšReinforcement Learningæ–¹å‘çš„ï¼Œè¦æ˜ç¡®å…¶ç›®æ ‡: **æ‰¾åˆ°å¯ä»¥è®©agentè·å¾—æœ€ä¼˜å›æŠ¥çš„æœ€ä¼˜è¡Œä¸ºç­–ç•¥ $\pi^*$**ï¼Œæ‰€ä»¥å¯¹ç­–ç•¥ç›´æ¥è¿›è¡Œå»ºæ¨¡å¹¶æŒ‰ç…§æ¢¯åº¦æå‡å°±æ˜¯ä¸€ä¸ªå¾ˆè‡ªç„¶çš„æƒ³æ³•äº†ã€‚

## Vanilla Policy Gradient
Policy gradientè¾“å‡ºä¸æ˜¯ action çš„ value, è€Œæ˜¯å…·ä½“çš„é‚£ä¸€ä¸ª action, è¿™æ · policy gradient å°±è·³è¿‡äº† value è¯„ä¼°è¿™ä¸ªé˜¶æ®µ, å¯¹ç­–ç•¥æœ¬èº«è¿›è¡Œè¯„ä¼°ã€‚

### Theory

$$
\pi_\theta(a|s)=P[a|s]
$$

We must find the best parameters (Î¸) to maximize a score function, J(Î¸).
$$
J(\theta)=E_{\pi_\theta}[\sum\gamma r]
$$
There are two steps:

- Measure the quality of a Ï€ (policy) with a **policy score function** J(Î¸) (ç­–ç•¥è¯„ä¼°)
- Use **policy gradient ascent** to find the best parameter Î¸ that improves our Ï€. (ç­–ç•¥æå‡)

### Policy score function

- episode environment with same start state $s_1$
   $$
   J_1(\theta)=E_\pi[G_1=R_1+\gamma R_2+\gamma^2 R_3+\dots]=E_\pi (V(s_1))
   $$

- continuous environment (use the average value) 

	$$\begin{aligned}
        J_{avgv}(\theta)&=E_{\pi}(V(s))=\sum_{s\in \mathcal{S}} d^\pi (s)V^\pi(s)\\&=\sum_{s\in \mathcal{S}} d^\pi (s) \sum_{a\in \mathcal{A}} \pi_\theta(a|s)Q^\pi(s,a)
    \end{aligned}
    $$

	where $d^\pi (s)=\dfrac{N(s)}{\sum_{s'}N(s')}$, $N(s)$ means Number of occurrences of the state, $\sum_{s'}N(s')$ represents Total number of occurrences of all state. So $d^\pi (s)$ ä»£è¡¨åœ¨ç­–ç•¥ $\pi_\theta$ ä¸‹é©¬å°”ç§‘å¤«é“¾çš„å¹³ç¨³åˆ†å¸ƒ (on-policy state distribution under Ï€), è¯¦è§[Policy Gradient Algorithms - lilianweng's blog](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html)ğŸ‘
	
- use the average reward per time step. The idea here is that we want to get the most reward per time step.

   ![image-20191205092649257](../img/Reinforcement%20Learning%20Notes.assets/image-20191205092649257.png)

### Policy gradient asscent
ä¸æˆ‘ä»¬æƒ¯ç”¨çš„æ¢¯åº¦ä¸‹é™ç›¸åï¼Œè¿™é‡Œç”¨çš„æ˜¯**æ¢¯åº¦ä¸Šå‡**ï¼
$$
\theta\leftarrow \theta + \alpha\nabla_\theta J(\theta)
$$

$$
\theta^*=\arg\max_\theta \underbrace{E_{\pi \theta}[\sum_t R(s_t,a_t)]}_{J(\theta)}
$$

Our score function J(Î¸) can be also defined as:

![image-20191205102211427](../img/Reinforcement%20Learning%20Notes.assets/image-20191205102211427.png)

Since $J(Î¸)$ is composed of state distribution and action distribution, when we gradient with respect to $\theta$, the effect of action is simple to find but the state effect is much more complicated due to the unknown environment. The solution is to use **Policy Gradient Theorem**:

æˆ‘ä»¬å°†ä¸Šä¸€èŠ‚çš„ä¸‰ç§policy score functionå½’çº³ä¸ºï¼š

![image-20191205103636621](../img/Reinforcement%20Learning%20Notes.assets/image-20191205103636621.png)


It provides a nice reformation of the derivative of the objective function to not involve the derivative of the state distribution $d_Ï€(.)$ and simplify the gradient computation $âˆ‡_Î¸J(Î¸)$ a lot.

$$
\begin{aligned}
\nabla_\theta J(\theta)&=\nabla_\theta \sum_{s \in \mathcal{S}} d^{\pi}(s)\sum_\tau \pi(\tau;\theta)R(\tau)\\
&\propto\sum_{s \in \mathcal{S}} d^{\pi}(s)\sum_\tau \nabla_\theta \pi(\tau;\theta)R(\tau)
\end{aligned}
$$

**Proof:** [Policy Gradient Algorithms - lilianweng's blog](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html)ğŸ‘

It is also hard to differentiating $\pi$, unless we can transform it into a **logarithm**. ([likelihood ratio trick](http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/))

![image-20191205103234091](../img/Reinforcement%20Learning%20Notes.assets/image-20191205103234091.png)

åˆ†è§£ $log\pi_\theta(\tau)$, å»æ‰ä¸å½±å“åå¯¼çš„æ— å…³é¡¹, å°±å¯ä»¥å¾—åˆ°åªä¸å½“å‰åŠ¨ä½œ-çŠ¶æ€å¯¹æœ‰å…³çš„[æœ€å¤§ä¼¼ç„¶ä¼°è®¡](https://zhuanlan.zhihu.com/p/26614750).

![](../img/Reinforcement%20Learning%20Notes.assets/å¾®ä¿¡æˆªå›¾_20200430093636.png)



é‚£ä¹ˆè¿™ä¸ªlogçš„åå¯¼æ€ä¹ˆæ±‚å‘¢?

![](../img/Reinforcement%20Learning%20Notes.assets/å¾®ä¿¡æˆªå›¾_20200430100118.png)

åœ¨Codingçš„æ—¶å€™å°±æ˜¯è¿™æ®µ:

```
y = np.zeros([self.act_space])
y[act] = 1 # åˆ¶ä½œç¦»æ•£åŠ¨ä½œç©ºé—´ï¼Œæ‰§è¡Œäº†çš„ç½®1
self.gradients.append(np.array(y).astype('float32')-prob)
```

æœ€å, æˆ‘ä»¬å¾—åˆ°äº†VPGçš„æ›´æ–°æ–¹æ³•:

![image-20191205103810941](../img/Reinforcement%20Learning%20Notes.assets/image-20191205103810941.png)

å¯¹åº”çš„codeå°±æ˜¯, è¿™é‡Œå¯¹rewardåšäº†å½’ä¸€åŒ–:

```
def learn(self):
    gradients = np.vstack(self.gradients)
    rewards = np.vstack(self.rewards)
    rewards = self.discount_rewards(rewards)
    # rewardå½’ä¸€åŒ–
    rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)
    gradients *= rewards
    X = np.squeeze(np.vstack([self.states]))
    Y = self.act_probs + self.alpha * np.squeeze(np.vstack([gradients]))
```

### Pseudocode

REINFORCE: ä¸€ç§åŸºäºæ•´æ¡å›åˆæ•°æ®çš„æ›´æ–°, remember that? Monte-Carlo method!

![Policy Gradients ç®—æ³•æ›´æ–° (./img/5-1-1.png)](https://morvanzhou.github.io/static/results/reinforcement-learning/5-1-1.png)

> å…¶ä¸­ï¼Œ$\nabla log \pi_{\theta}(s_t,a_t)v_t$å¯ä»¥ç†è§£ä¸ºåœ¨çŠ¶æ€ $s$å¯¹æ‰€é€‰åŠ¨ä½œçš„ $a$ çš„åƒæƒŠåº¦ï¼Œ$\pi_{\theta}(s_t,a_t)$æ¦‚ç‡è¶Šå°ï¼Œåå‘çš„ $log(Policy(s,a))$(å³ `-log(P)`) åè€Œè¶Šå¤§. å¦‚æœåœ¨ `Policy(s,a)` å¾ˆå°çš„æƒ…å†µä¸‹, æ‹¿åˆ°äº†ä¸€ä¸ªå¤§çš„ `R`, ä¹Ÿå°±æ˜¯å¤§çš„ `V`, é‚£ $\nabla log \pi_{\theta}(s_t,a_t)v_t$ å°±æ›´å¤§, è¡¨ç¤ºæ›´åƒæƒŠ, (**æˆ‘é€‰äº†ä¸€ä¸ªä¸å¸¸é€‰çš„åŠ¨ä½œ, å´å‘ç°åŸæ¥å®ƒèƒ½å¾—åˆ°äº†ä¸€ä¸ªå¥½çš„ reward, é‚£æˆ‘å°±å¾—å¯¹æˆ‘è¿™æ¬¡çš„å‚æ•°è¿›è¡Œä¸€ä¸ªå¤§å¹…ä¿®æ”¹**). è¿™å°±æ˜¯åƒæƒŠåº¦çš„ç‰©ç†æ„ä¹‰.

### Implement
```
'''
ç”¨äºå›åˆæ›´æ–°çš„ç¦»æ•£æ§åˆ¶
'''
class Skylark_VPG():
    def __init__(self, env, alpha = 0.1, gamma = 0.6, epsilon=0.1, update_freq = 200):
        self.obs_space = 80*80  # è§†æ ¹æ®å…·ä½“gymç¯å¢ƒçš„stateè¾“å‡ºæ ¼å¼ï¼Œå…·ä½“åˆ†æ
        self.act_space = env.action_space.n
        self.env = env
        self.alpha = alpha      # learning rate
        self.gamma = gamma      # discount rate
        self.states = []
        self.gradients = []
        self.rewards = []
        self.act_probs = []
        self.total_step = 0

        self.model = self._build_model()
        self.model.summary()

    def _build_model(self):
        model = Sequential()
        model.add(Reshape((1, 80, 80), input_shape=(self.obs_space,)))
        model.add(Conv2D(32, (6, 6), activation="relu", strides=(3, 3), 
                        padding="same", kernel_initializer="he_uniform"))
        model.add(Flatten())
        model.add(Dense(64, activation='relu', kernel_initializer='he_uniform'))
        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))
        # softmaxç­–ç•¥ä½¿ç”¨æè¿°çŠ¶æ€å’Œè¡Œä¸ºçš„ç‰¹å¾Ï•(s,a) ä¸å‚æ•°\thetaçš„çº¿æ€§ç»„åˆæ¥æƒè¡¡ä¸€ä¸ªè¡Œä¸ºå‘ç”Ÿçš„æ¦‚ç‡
        # è¾“å‡ºä¸ºæ¯ä¸ªåŠ¨ä½œçš„æ¦‚ç‡
        model.add(Dense(self.act_space, activation='softmax'))
        opt = Adam(lr=self.alpha)
        model.compile(loss='categorical_crossentropy', optimizer=opt)
        return model
    
    def choose_action(self, state):
        state = state.reshape([1, self.obs_space])
        act_prob = self.model.predict(state).flatten()
        prob = act_prob / np.sum(act_prob)
        self.act_probs.append(act_prob)
        # æŒ‰æ¦‚ç‡é€‰å–åŠ¨ä½œ
        action = np.random.choice(self.act_space, 1, p=prob)[0]
        return action, prob
        
    def store_trajectory(self, s, a, r, prob):
        y = np.zeros([self.act_space])
        y[a] = 1 # åˆ¶ä½œç¦»æ•£åŠ¨ä½œç©ºé—´ï¼Œæ‰§è¡Œäº†çš„ç½®1
        self.gradients.append(np.array(y).astype('float32')-prob)
        self.states.append(s)
        self.rewards.append(r)

    def discount_rewards(self, rewards):
        '''
        ä»å›åˆç»“æŸä½ç½®å‘å‰ä¿®æ­£reward
        '''
        discounted_rewards = np.zeros_like(rewards)
        running_add = 0
        for t in reversed(range(0, rewards.size)):
            if rewards[t] != 0:
                running_add = 0
            running_add = running_add * self.gamma + rewards[t]
            discounted_rewards[t] = np.array(running_add)
        return discounted_rewards

    def learn(self):
        gradients = np.vstack(self.gradients)
        rewards = np.vstack(self.rewards)
        rewards = self.discount_rewards(rewards)
        # rewardå½’ä¸€åŒ–
        rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-7)
        gradients *= rewards
        X = np.squeeze(np.vstack([self.states]))
        Y = self.act_probs + self.alpha * np.squeeze(np.vstack([gradients]))
        self.model.train_on_batch(X, Y)
        self.states, self.act_probs, self.gradients, self.rewards = [], [], [], []

    def train(self, num_episodes, batch_size = 128, num_steps = 100):
        for i in range(num_episodes):
            state = self.env.reset()

            steps, penalties, reward, sum_rew = 0, 0, 0, 0
            done = False
            while not done:
                # self.env.render()
                state = preprocess(state)
                action, prob = self.choose_action(state)
                # Interaction with Env
                next_state, reward, done, info = self.env.step(action) 
                
                self.store_trajectory(state, action, reward, prob)

                sum_rew += reward
                state = next_state
                steps += 1
                self.total_step += 1
            if done:
                self.learn()
                print('Episode: {} | Avg_reward: {} | Length: {}'.format(i, sum_rew/steps, steps))
        print("Training finished.")
```

### Feature
**Advantages**

1. è¾“å‡ºçš„è¿™ä¸ª action å¯ä»¥æ˜¯ä¸€ä¸ª**è¿ç»­å€¼**, ä¹‹å‰æˆ‘ä»¬è¯´åˆ°çš„ value-based æ–¹æ³•è¾“å‡ºçš„éƒ½æ˜¯ä¸è¿ç»­çš„å€¼, ç„¶åå†é€‰æ‹©å€¼æœ€å¤§çš„ action. è€Œ policy gradient å¯ä»¥åœ¨ä¸€ä¸ªè¿ç»­åˆ†å¸ƒä¸Šé€‰å– action.

2. Convergence: The problem with value-based methods is that they can have a big oscillation while training. This is because the choice of action may change dramatically for an arbitrarily small change in the estimated action values.

   On the other hand, with policy gradient, we just follow the gradient to find the best parameters. We see a smooth update of our policy at each step.

   Because we follow the gradient to find the best parameters, weâ€™re guaranteed to converge on a local maximum (worst case) or global maximum (best case).

3. Policy gradients can learn stochastic policies

   - we donâ€™t need to implement an exploration/exploitation trade off.

   -  get rid of the problem of perceptual aliasing.

**Disadvantages**

1. A lot of the time, they converge on a local maximum rather than on the global optimum.
2. In a situation of Monte Carlo, waiting until the end of episode to calculate the reward.

## TRPO (Trust Region Policy Optimization)
TRPOè¯‘ä¸º**ä¿¡èµ–åŸŸç­–ç•¥ä¼˜åŒ–**ï¼ŒTRPOçš„å‡ºç°æ˜¯è¦è§£å†³VPGå­˜åœ¨çš„é—®é¢˜çš„ï¼š**VPGçš„æ›´æ–°æ­¥é•¿ $\alpha$ æ˜¯ä¸ªå›ºå®šå€¼ï¼Œå¾ˆå®¹æ˜“äº§ç”Ÿä»ä¸€ä¸ªä¸å¥½çš„ç­–ç•¥'æå‡'åˆ°å¦ä¸€ä¸ªæ›´å·®çš„ç­–ç•¥ä¸Šã€‚**

è¿™è®©æˆ‘æƒ³èµ·äº†ä¼˜åŒ–ä¸­å¯¹æ­¥é•¿çš„ä¼°è®¡ï¼šArmijo-Goldsteinå‡†åˆ™ã€Wolfe-Powellå‡†åˆ™ç­‰ã€‚å½“ç„¶å’ŒTRPOå…³ç³»ä¸å¤§ã€‚

TRPOæœ‰ä¸€ä¸ªå¤§èƒ†çš„æƒ³æ³•ï¼Œè¦**è®©æ›´æ–°åçš„ç­–ç•¥å›æŠ¥å‡½æ•°å•è°ƒä¸å‡**ã€‚ä¸€ä¸ªè‡ªç„¶çš„æƒ³æ³•æ˜¯ï¼Œ**å°†æ–°ç­–ç•¥æ‰€å¯¹åº”çš„å›æŠ¥å‡½æ•°è¡¨ç¤ºæˆæ—§ç­–ç•¥æ‰€å¯¹åº”çš„å›æŠ¥å‡½æ•°+å…¶ä»–é¡¹**ã€‚ä¸‹å¼å°±æ˜¯TRPOçš„èµ·æ‰‹å¼ï¼š

$$\eta(\hat{\pi})=\eta(\pi)+E_{s_{0}, a_{0}, \cdots \hat{\pi}}\left[\sum_{t=0}^{\infty} \gamma^{t} A_{\pi}\left(s_{t}, a_{t}\right)\right]$$

å…¶ä¸­ï¼Œ$A_\pi$ä¸ºä¼˜åŠ¿å‡½æ•°([è¿™ä¸ªä¼šåœ¨A2Cçš„ç« èŠ‚è®²åˆ°]())

$$\begin{aligned}
A_{\pi}(s, a)&=Q_{\pi}(s, a)-V_{\pi}(s) \\
&=E_{s^{\prime}\sim P\left(s^{\prime}| s, a\right)}  \left[r(s)+\gamma V^{\pi}\left(s^{\prime}\right)-V^{\pi}(s)\right]
\end{aligned}$$

> **Proof:**  (ä¹Ÿå¯ä»¥é€šè¿‡æ„é€ æ³•åæ¨)
> $$\begin{aligned}
E_{\tau | \tilde{\pi}}\left[\sum_{t=0}^{\infty} \gamma^{t} A_{\pi}\left(s_{t}, a_{t}\right)\right] 
&=E_{\tau | \tilde{\pi}}\left[\sum_{t=0}^{\infty} \gamma^{t}\left(r(s)+\gamma V^{\pi}\left(s_{t+1}\right)-V^{\pi}\left(s_{t}\right)\right)\right] \\
&=E_{\tau | \tilde{\pi}}\left[\sum_{t=0}^{\infty} \gamma^{t}\left(r\left(s_{t}\right)\right)+\sum_{t=0}^{\infty} \gamma^{t}\left(\gamma V^{\pi}\left(s_{t+1}\right)-V^{\pi}\left(s_{t}\right)\right)\right] \\
&=E_{\tau | \tilde{\pi}}\left[\sum_{t=0}^{\infty} \gamma^{t}\left(r\left(s_{t}\right)\right)\right]+E_{s_{0}}\left[-V^{\pi}\left(s_{0}\right)\right] \\
&=\eta(\tilde{\pi})-\eta(\pi)
\end{aligned}$$


ç”±æ­¤ï¼Œæˆ‘ä»¬å°±å®ç°äº†å°†æ–°ç­–ç•¥çš„å›æŠ¥è¡¨ç¤ºä¸ºæ—§ç­–ç•¥å›æŠ¥çš„ç›®æ ‡ã€‚

æœ‰äº†èµ·æ‰‹å¼ï¼Œæˆ‘ä»¬åœ¨å®é™…æ“ä½œæ—¶å€™å…·ä½“æ€ä¹ˆè®¡ç®—å‘¢ï¼Ÿå°¤å…¶æ˜¯ä¼˜åŠ¿å‡½æ•°å¤–é‚£ä¸ªæœŸæœ›çš„æ€ä¹ˆå¤„ç†ï¼Ÿ


å°†å…¶åˆ†è§£æˆstateå’Œactionçš„æ±‚å’Œï¼š

$$\eta(\hat{\pi})=\eta(\pi)+\sum_{t=0}^{\infty} \sum_{s} P\left(s_{t}=s | \hat{\pi}\right) \sum_{a} \hat{\pi}(a | s) \gamma^{t} A_{\pi}(s, a)$$

## PPO (Proximal Policy Optimization)

### Theory

**The central idea of Proximal Policy Optimization is to avoid having too large policy update.** To do that, we use a ratio that will tells us the difference between our new and old policy and clip this ratio from 0.8 to 1.2. Doing that will ensure **that our policy update will not be too large.**

The problem comes from the step size of gradient ascent:

- Too small, **the training process was too slow**
- Too high, **there was too much variability in the training.**

The idea is that PPO improves the stability of the Actor training by limiting the policy update at each training step.

To be able to do that PPO introduced a new objective function called â€œ**Clipped surrogate objective function**â€ that **will constraint the policy change in a small range using a clip.**

Instead of using log pi to trace the impact of the actions, we can use **the ratio between the probability of action under current policy divided by the probability of the action under previous policy.**
$$
r_t(\theta)=\dfrac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}, \text{so } r(\theta_{old})=1
$$

- If $r_t(Î¸)$ >1, it means that the **action is more probable in the current policy than the old policy.**
- If $r_t(Î¸)$ is between 0 and 1: it means that the **action is less probable for current policy than for the old one.**

As consequence, our new objective function could be:
$$
L^{CPI}(\theta)=\hat{\mathbb{E}}_t\lbrack\dfrac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}\hat{A}_t\rbrack=\hat{\mathbb{E}}_t[r_t(\theta)\hat{A}_t]
$$
**By doing that weâ€™ll ensure that not having too large policy update because the new policy canâ€™t be too different from the older one.**

To do that we have two solutions:

- TRPO (Trust Region Policy Optimization) uses KL divergence constraints outside of the objective function to constraint the policy update. But this method **is much complicated to implement and it takes more computation time.**
- PPO clip probability ratio directly in the objective function with its Clipped surrogate objective function.

![image-20191205121930328](../img/Reinforcement%20Learning%20Notes.assets/image-20191205121930328.png)

The final Clipped Surrogate(ä»£ç†) Objective Loss:

![image-20191205190844049](../img/Reinforcement%20Learning%20Notes.assets/image-20191205190844049.png)

### Feature
**Advantage**

It can be used in both discrete and continuous control.

**Disadvantage**

on-policy -> data inefficient



## Reference 
1. [Policy Gradients - è«çƒ¦](https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/5-1-A-PG/)
2. [Policy Gradient Algorithms - lilianweng's blog](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html)ğŸ‘
3. [An introduction to Policy Gradients with Cartpole and Doom](https://www.freecodecamp.org/news/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f/)